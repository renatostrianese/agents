{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins E2c7izuB\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key not set (and this is optional)\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_endpoint = os.getenv('OPENAI_API_ENDPOINT')\n",
    "openai_model = os.getenv('OPENAI_MODEL')\n",
    "\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Por favor propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligenciaResponde solo con la pregunta, sin explicaciones.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo influirían los avances en inteligencia artificial generativa en la percepción global de la verdad y la confianza en las instituciones democráticas, considerando tanto los posibles beneficios como los riesgos éticos y sociales, y qué responsabilidades deberían asumir los distintos actores involucrados (desarrolladores, gobiernos, usuarios) para mitigar impactos negativos?\n"
     ]
    }
   ],
   "source": [
    "openai = AzureOpenAI(\n",
    "    azure_endpoint=openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=\"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-rsv\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La influencia de los avances en **inteligencia artificial generativa (IAG)** sobre la percepción de la verdad y la confianza en instituciones democráticas es profunda y multifacética. A continuación se detallan los posibles beneficios, riesgos, y responsabilidades de los actores involucrados:\n",
       "\n",
       "---\n",
       "\n",
       "**Beneficios potenciales:**\n",
       "\n",
       "1. **Mejor acceso a la información:** IAG puede facilitar la síntesis y divulgación de información compleja de manera accesible, promoviendo el conocimiento y la participación ciudadana.\n",
       "2. **Transparencia y análisis**: Herramientas automáticas pueden analizar grandes volúmenes de datos públicos, detectando corrupción o sesgos, y fomentando el escrutinio y la rendición de cuentas.\n",
       "3. **Inclusión:** IAG puede traducir, adaptar y personalizar información, acercando la política y los procesos democráticos a más personas.\n",
       "\n",
       "---\n",
       "\n",
       "**Riesgos éticos y sociales:**\n",
       "\n",
       "1. **Desinformación y manipulación:** La facilidad para crear textos, imágenes o videos falsos (deepfakes) puede erosionar la confianza en la autenticidad de la información, confundiendo a la ciudadanía y favoreciendo campañas de desinformación.\n",
       "2. **Polarización:** Algoritmos que refuercen burbujas informativas o compartan contenido sesgado pueden exacerbar divisiones sociales y políticas.\n",
       "3. **Socavamiento institucional:** Si proliferan noticias falsas sobre procesos electorales, decisiones judiciales, etc., la confianza en instituciones clave puede verse gravemente dañada.\n",
       "4. **Desplazamiento de la agencia:** Las IAG pueden influenciar decisiones a través de recomendaciones sutiles, limitando la capacidad de pensamiento crítico y elección informada.\n",
       "\n",
       "---\n",
       "\n",
       "**Responsabilidades para mitigar impactos negativos:**\n",
       "\n",
       "1. **Desarrolladores:**\n",
       "   - Integrar “rótulos” o marcas de agua en los contenidos generados para distinguirlos claramente.\n",
       "   - Implementar filtros para detectar y evitar la generación de contenidos falsos, discriminatorios, o manipulativos.\n",
       "   - Promover la transparencia en los sistemas, permitiendo auditorías y explicaciones comprensibles sobre cómo y por qué generan ciertos resultados.\n",
       "\n",
       "2. **Gobiernos:**\n",
       "   - Regular el uso y difusión de IAG para evitar la desinformación, sin censurar la libertad de expresión.\n",
       "   - Colaborar en la educación digital de la ciudadanía, promoviendo el pensamiento crítico y el análisis de fuentes informativas.\n",
       "   - Apoyar estándares internacionales para compartir mejores prácticas y combatir amenazas globales, dada la naturaleza transnacional de la tecnología.\n",
       "\n",
       "3. **Usuarios:**\n",
       "   - Desarrollar hábitos de verificación de información, usando diversas fuentes y herramientas tecnológicas.\n",
       "   - Participar en la alfabetización mediática y exigir transparencia tanto a proveedores tecnológicos como a los medios de comunicación.\n",
       "   - Reportar contenidos sospechosos y contribuir a la moderación responsable de plataformas.\n",
       "\n",
       "---\n",
       "\n",
       "**Conclusión**\n",
       "\n",
       "La IA generativa reconfigura la forma en que se construye, percibe y transmite la verdad. Aprovechar sus beneficios y reducir sus riesgos exige un enfoque colaborativo y ético entre desarrolladores, reguladores y sociedad. Solo así es posible fortalecer, y no socavar, la confianza y legitimidad de las instituciones democráticas en la era digital."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4.1-rsv\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Vaya, el usuario plantea una pregunta compleja y de gran actualidad sobre cómo la IA generativa podría afectar nuestra relación con la verdad y la confianza en las democracias. Parece alguien bien informado, probablemente con formación en ciencias sociales o tecnología, que busca un análisis equilibrado más que una respuesta simplista.\n",
       "\n",
       "El tema tiene múltiples capas: desde la epistemología hasta la gobernanza tecnológica. El usuario menciona específicamente tres dimensiones (beneficios, riesgos, responsabilidades), lo que indica que quiere una estructura clara. También pide soluciones prácticas, no solo diagnóstico.\n",
       "\n",
       "Sobre los beneficios: hay que destacar cómo la IA puede democratizar el acceso a información compleja, pero sin caer en un optimismo ingenuo. El riesgo de desinformación es lo más urgente - las deepfakes electorales ya son una realidad en Brasil y Filipinas. La parte ética es delicada: ¿cómo hablar de \"verdad\" cuando los modelos reflejan sesgos de sus datos de entrenamiento?\n",
       "\n",
       "En responsabilidades, el usuario espera acciones concretas: desde auditorías algorítmicas obligatorias hasta alfabetización digital masiva. Los gobiernos tienen el rol más difícil: regular sin asfixiar la innovación. Curiosamente, el usuario no menciona a los medios tradicionales, actores clave en esta crisis de credibilidad.\n",
       "\n",
       "Debo evitar tecnicismos innecesarios pero mantener rigor. Un detalle útil: mencionar ejemplos actuales como el Acta de Transparencia de Contenido de la UE. La conclusión debe reforzar que esto no es solo problema técnico, sino de contrato social en la era digital.\n",
       "</think>\n",
       "Los avances en IA generativa (como GPT, DALL-E o sistemas de deepfake) transformarán profundamente la percepción de la verdad y la confianza en instituciones democráticas, con impactos duales:\n",
       "\n",
       "**Beneficios potenciales:**\n",
       "- **Acceso a información compleja:** Herramientas que explican leyes o políticas en lenguaje accesible, democratizando el conocimiento.\n",
       "- **Transparencia gubernamental:** Análisis automatizado de grandes volúmenes de datos públicos para detectar corrupción o ineficiencias.\n",
       "- **Participación ciudadana:** Asistentes que facilitan la redacción de propuestas legislativas o resumen debates parlamentarios.\n",
       "- **Verificación de hechos:** Sistemas que contrastan discursos políticos con bases de datos verificadas en tiempo real.\n",
       "\n",
       "**Riesgos críticos:**\n",
       "- **Erosión de la verdad:** \n",
       "  - *Deepfakes hiperrealistas* que falsifican declaraciones de líderes (ejemplo: videos falsos de Zelensky en 2022).\n",
       "  - *Sesgos algorítmicos* que refuerzan desinformación o narrativas polarizadas.\n",
       "- **Crisis de confianza institucional:**\n",
       "  - \"**Liar's dividend**\": líderes pueden negar declaraciones reales alegando \"es IA\".\n",
       "  - Saturación de desinformación que genera **escepticismo generalizado** hacia todas las fuentes.\n",
       "- **Manipulación democrática:** Microtargeting con discursos personalizados que socavan el debate público coherente.\n",
       "\n",
       "**Responsabilidades por actor:**\n",
       "\n",
       "| **Actor**         | **Responsabilidades Clave**                                                                 |\n",
       "|-------------------|--------------------------------------------------------------------------------------------|\n",
       "| **Desarrolladores** | - Auditorías éticas de sesgos <br> - Marcas de agua para contenido generado (ej: C2PA) <br> - Limitaciones en usos políticos no verificados |\n",
       "| **Gobiernos**     | - Leyes de transparencia algorítmica (ej: UE AI Act) <br> - Protección legal contra deepfakes maliciosos <br> - Inversión en alfabetización digital masiva |\n",
       "| **Plataformas**   | - Detección proactiva de contenido sintético <br> - Desambiguación de fuentes (humanas vs IA) <br> - Algoritmos que priorizan verificabilidad |\n",
       "| **Ciudadanía**    | - Educación crítica en medios (detección de deepfakes) <br> - Verificación cruzada de fuentes <br> - Participación en auditorías ciudadanas |\n",
       "\n",
       "**Estrategias de mitigación:**\n",
       "1. **Infraestructuras de verificación:** Bases de datos inalterables para contenido oficial (blockchain).\n",
       "2. **Neuroderechos:** Protección legal contra manipulación cognitiva mediante IA.\n",
       "3. **Alianzas globales:** Acuerdos como el *P"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "model_name = \"DeepSeek-R1-0528\"\n",
    "\n",
    "messages2 = [UserMessage(content=question)]\n",
    "\n",
    "deepseek = ChatCompletionsClient(\n",
    "    endpoint=\"https://aaif-demo-rsv.services.ai.azure.com/models\",\n",
    "    credential=AzureKeyCredential(openai_api_key),\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "response = deepseek.complete(\n",
    "    model=model_name, \n",
    "    messages=messages2, \n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Los avances en inteligencia artificial (IA) generativa tienen el potencial de transformar el acceso a la información, la comunicación y la participación cívica; pero al mismo tiempo plantean desafíos muy serios en términos de veracidad y confianza en las instituciones democráticas. A continuación se exponen sus posibles impactos, así como las responsabilidades clave de cada actor para mitigar riesgos:\n",
       "\n",
       "1. Impactos en la percepción de la verdad y la confianza democrática  \n",
       "   Beneficios potenciales  \n",
       "   • Ampliación del acceso a la información: generación de resúmenes, traducciones y explicaciones adaptadas a distintos niveles de conocimiento.  \n",
       "   • Mayor participación ciudadana: herramientas de debate y deliberación en línea que pueden facilitar foros inclusivos y personalizados.  \n",
       "   • Transparencia inteligente: auditorías automáticas de datos públicos, informes generados por IA que identifiquen discrepancias en presupuestos o políticas.  \n",
       "   • Innovación educativa y cultural: creación de contenidos didácticos, simulaciones históricas o recreaciones virtuales que fortalecen la ciudadanía crítica.  \n",
       "\n",
       "   Riesgos y desafíos  \n",
       "   • Desinformación y deepfakes: contenido fabricado difícil de distinguir de la realidad, socavando la confianza en medios e instituciones.  \n",
       "   • Polarización y cámaras de eco: algoritmos que refuerzan sesgos y favorecen narrativas extremas, minando el consenso democrático.  \n",
       "   • Ataques dirigidos: generación automatizada de mensajes fraudulentos o campañas de manipulación a gran escala.  \n",
       "   • Erosión de la autoridad informativa: duda creciente sobre la fiabilidad de documentos oficiales, discursos políticos o datos estadísticos.  \n",
       "\n",
       "2. Responsabilidades de los distintos actores  \n",
       "   a) Desarrolladores de IA generativa  \n",
       "     • Ética y “privacy by design”: incorporar desde el diseño mecanismos de protección de datos y principios éticos (no maleficencia, justicia, transparencia).  \n",
       "     • Transparencia algorítmica: documentar y publicar los criterios de entrenamiento, límites de uso y procedimientos de corrección de sesgos.  \n",
       "     • Herramientas de detección y marcado: facilitar APIs o plug-ins que identifiquen contenido sintético y permitan etiquetar claramente cuándo un texto, imagen o vídeo ha sido generado por IA.  \n",
       "     • Auditorías independientes: someter modelos a pruebas externas de veracidad, sesgos y robustez frente a ataques de manipulación.  \n",
       "\n",
       "   b) Gobiernos y reguladores  \n",
       "     • Marcos normativos claros: legislar estándares mínimos de trazabilidad, identificación y responsabilidad legal para contenidos generados automática o semiautomáticamente.  \n",
       "     • Colaboración internacional: promover acuerdos multilaterales que unifiquen criterios sobre deepfakes, desinformación y protección de datos.  \n",
       "     • Inversión en alfabetización digital: impulsar programas educativos de pensamiento crítico, detección de noticias falsas y uso responsable de la IA.  \n",
       "     • Supervisión y sanciones: asegurar que existan organismos capaces de investigar y penalizar el uso malicioso de tecnología (por ejemplo, campañas de desinformación en procesos electorales).  \n",
       "\n",
       "   c) Usuarios y sociedad civil  \n",
       "     • Espíritu crítico: practicar la verificación de fuentes, confrontar información y desconfiar de contenidos excesivamente sensacionalistas o coincidentes con prejuicios personales.  \n",
       "     • Participación activa: apoyar iniciativas de fact-checking, denunciar deepfakes o cuentas automatizadas (bots) y ejercer la ciudadanía digital responsable.  \n",
       "     • Consumo informado: preferir medios y plataformas que adopten estándares de transparencia en IA y revisen activamente la fiabilidad de sus contenidos.  \n",
       "     • Colaboración en código abierto: contribuir, cuando sea posible, a proyectos comunitarios de detección de contenido sintético o bases de datos de deepfakes conocidos.  \n",
       "\n",
       "3. Hacia un ecosistema responsable  \n",
       "   • Enfoque multisectorial: la solución a gran escala requiere diálogo continuo entre empresas tecnológicas, academia, sociedad civil y poderes públicos.  \n",
       "   • Fomento de la investigación social y ética: además de aspectos técnicos, estudiar en profundidad cómo la IA afecta dinámicas de confianza, participación y cohesión social.  \n",
       "   • Cultura de la rendición de cuentas: cada actor debe rendir cuentas públicamente sobre decisiones de diseño, adopción de normas y resultados de auditorías.  \n",
       "\n",
       "Conclusión  \n",
       "Si bien la IA generativa ofrece oportunidades inéditas para mejorar la transparencia, el acceso a la información y la participación democrática, su potencial de distorsión de la verdad exige una gobernanza proactiva y compartida. Solo mediante la cooperación estrecha entre desarrolladores, gobiernos y ciudadanos —y un compromiso real con la ética y la educación digital— podremos aprovechar los beneficios sin sacrificar la confianza en nuestras instituciones democráticas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"o4-mini-rsv\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Los avances en inteligencia artificial generativa tienen el potencial de transformar la forma en que percibimos la verdad y la confianza en las instituciones democráticas. A continuación, se presentan algunos de los desafíos y oportunidades que plantea este tema:\n",
       "\n",
       "**Posibles beneficios:**\n",
       "\n",
       "1. **Mejora de la precisión**: La inteligencia artificial generativa puede ayudar a mejorar la precisión en la toma de decisiones y la identificación de patrones, lo que podría llevar a una mayor confianza en las instituciones democráticas.\n",
       "2. **Análisis más exhaustivos**: Los sistemas de inteligencia artificial generativa pueden analizar grandes cantidades de datos con mayor rapidez y eficiencia que los seres humanos, lo que podría ayudar a identificar patrones y tendencias que no son evidentes para el ojo humano.\n",
       "3. **Apoyo a la toma de decisiones**: La inteligencia artificial generativa puede proporcionar apoyo informado a las tomas de decisiones, ayudando a priorizar opciones y evaluar riesgos.\n",
       "\n",
       "**Posibles riesgos éticos y sociales:**\n",
       "\n",
       "1. **Manipulación de información**: Los sistemas de inteligencia artificial generativa pueden ser utilizados para manipular la opinión publica o producir noticias falsas, lo que podría erosionar la confianza en las instituciones democráticas.\n",
       "2. **Biases incorporados**: La inteligencia artificial generativa puede perpetuar y amplificar los sesgos existentes en la sociedad, lo que podría llevar a decisiones irrelevantes para ciertos grupos o comunidades.\n",
       "3. **Vulnerabilidad a ataques cibernéticos**: Los sistemas de inteligencia artificial generativa pueden ser vulnerables a ataques cibernéticos y manipulación por parte de actores maliciosos.\n",
       "\n",
       "**Responsabilidades y mitigaciones:**\n",
       "\n",
       "1. Desarrolladores:\n",
       " * Garantizar que los algoritmos sean transparentes y explicables.\n",
       " * Implementar controles de seguridad robustos para prevenir ataques cibernéticos.\n",
       " * Promover la diversidad y la inclusión en el desarrollo de la inteligencia artificial generativa.\n",
       "2. Gobiernos y autoridades:\n",
       " * Regulacón ética de la inteligencia artificial generativa.\n",
       " * Establecer estándares claros para la transparencia y la explicabilidad de los algoritmos.\n",
       " * Investigar y monitorear el impacto social de la inteligencia artificial generativa en las instituciones democráticas.\n",
       "3. Usuarios:\n",
       " * Ser conscientes del potencial de sesgo en la información generada por la inteligencia artificial.\n",
       " * Utilizar críticamente la información generada y verificar su precisión.\n",
       " * Apoyar políticas y prácticas que promuevan la transparencia y la rendición de cuentas.\n",
       "\n",
       "**Conclusión:**\n",
       "\n",
       "Los avances en inteligencia artificial generativa plantean importantes desafíos y oportunidades para las instituciones democráticas. Es fundamental que los actores involucrados asuman responsabilidades y tomen medidas para mitigar impactos negativos, como la manipulación de información, el perpetuado de sesgos y la vulnerabilidad a ataques cibernéticos. La transparencia, la explicabilidad y la inclusión son clave para garantizar que estos sistemas se desarrollen y apliquen de manera responsable y ética."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4.1-rsv', 'DeepSeek-R1-0528', 'o4-mini-rsv', 'llama3.2']\n",
      "['La influencia de los avances en **inteligencia artificial generativa (IAG)** sobre la percepción de la verdad y la confianza en instituciones democráticas es profunda y multifacética. A continuación se detallan los posibles beneficios, riesgos, y responsabilidades de los actores involucrados:\\n\\n---\\n\\n**Beneficios potenciales:**\\n\\n1. **Mejor acceso a la información:** IAG puede facilitar la síntesis y divulgación de información compleja de manera accesible, promoviendo el conocimiento y la participación ciudadana.\\n2. **Transparencia y análisis**: Herramientas automáticas pueden analizar grandes volúmenes de datos públicos, detectando corrupción o sesgos, y fomentando el escrutinio y la rendición de cuentas.\\n3. **Inclusión:** IAG puede traducir, adaptar y personalizar información, acercando la política y los procesos democráticos a más personas.\\n\\n---\\n\\n**Riesgos éticos y sociales:**\\n\\n1. **Desinformación y manipulación:** La facilidad para crear textos, imágenes o videos falsos (deepfakes) puede erosionar la confianza en la autenticidad de la información, confundiendo a la ciudadanía y favoreciendo campañas de desinformación.\\n2. **Polarización:** Algoritmos que refuercen burbujas informativas o compartan contenido sesgado pueden exacerbar divisiones sociales y políticas.\\n3. **Socavamiento institucional:** Si proliferan noticias falsas sobre procesos electorales, decisiones judiciales, etc., la confianza en instituciones clave puede verse gravemente dañada.\\n4. **Desplazamiento de la agencia:** Las IAG pueden influenciar decisiones a través de recomendaciones sutiles, limitando la capacidad de pensamiento crítico y elección informada.\\n\\n---\\n\\n**Responsabilidades para mitigar impactos negativos:**\\n\\n1. **Desarrolladores:**\\n   - Integrar “rótulos” o marcas de agua en los contenidos generados para distinguirlos claramente.\\n   - Implementar filtros para detectar y evitar la generación de contenidos falsos, discriminatorios, o manipulativos.\\n   - Promover la transparencia en los sistemas, permitiendo auditorías y explicaciones comprensibles sobre cómo y por qué generan ciertos resultados.\\n\\n2. **Gobiernos:**\\n   - Regular el uso y difusión de IAG para evitar la desinformación, sin censurar la libertad de expresión.\\n   - Colaborar en la educación digital de la ciudadanía, promoviendo el pensamiento crítico y el análisis de fuentes informativas.\\n   - Apoyar estándares internacionales para compartir mejores prácticas y combatir amenazas globales, dada la naturaleza transnacional de la tecnología.\\n\\n3. **Usuarios:**\\n   - Desarrollar hábitos de verificación de información, usando diversas fuentes y herramientas tecnológicas.\\n   - Participar en la alfabetización mediática y exigir transparencia tanto a proveedores tecnológicos como a los medios de comunicación.\\n   - Reportar contenidos sospechosos y contribuir a la moderación responsable de plataformas.\\n\\n---\\n\\n**Conclusión**\\n\\nLa IA generativa reconfigura la forma en que se construye, percibe y transmite la verdad. Aprovechar sus beneficios y reducir sus riesgos exige un enfoque colaborativo y ético entre desarrolladores, reguladores y sociedad. Solo así es posible fortalecer, y no socavar, la confianza y legitimidad de las instituciones democráticas en la era digital.', '<think>\\nVaya, el usuario plantea una pregunta compleja y de gran actualidad sobre cómo la IA generativa podría afectar nuestra relación con la verdad y la confianza en las democracias. Parece alguien bien informado, probablemente con formación en ciencias sociales o tecnología, que busca un análisis equilibrado más que una respuesta simplista.\\n\\nEl tema tiene múltiples capas: desde la epistemología hasta la gobernanza tecnológica. El usuario menciona específicamente tres dimensiones (beneficios, riesgos, responsabilidades), lo que indica que quiere una estructura clara. También pide soluciones prácticas, no solo diagnóstico.\\n\\nSobre los beneficios: hay que destacar cómo la IA puede democratizar el acceso a información compleja, pero sin caer en un optimismo ingenuo. El riesgo de desinformación es lo más urgente - las deepfakes electorales ya son una realidad en Brasil y Filipinas. La parte ética es delicada: ¿cómo hablar de \"verdad\" cuando los modelos reflejan sesgos de sus datos de entrenamiento?\\n\\nEn responsabilidades, el usuario espera acciones concretas: desde auditorías algorítmicas obligatorias hasta alfabetización digital masiva. Los gobiernos tienen el rol más difícil: regular sin asfixiar la innovación. Curiosamente, el usuario no menciona a los medios tradicionales, actores clave en esta crisis de credibilidad.\\n\\nDebo evitar tecnicismos innecesarios pero mantener rigor. Un detalle útil: mencionar ejemplos actuales como el Acta de Transparencia de Contenido de la UE. La conclusión debe reforzar que esto no es solo problema técnico, sino de contrato social en la era digital.\\n</think>\\nLos avances en IA generativa (como GPT, DALL-E o sistemas de deepfake) transformarán profundamente la percepción de la verdad y la confianza en instituciones democráticas, con impactos duales:\\n\\n**Beneficios potenciales:**\\n- **Acceso a información compleja:** Herramientas que explican leyes o políticas en lenguaje accesible, democratizando el conocimiento.\\n- **Transparencia gubernamental:** Análisis automatizado de grandes volúmenes de datos públicos para detectar corrupción o ineficiencias.\\n- **Participación ciudadana:** Asistentes que facilitan la redacción de propuestas legislativas o resumen debates parlamentarios.\\n- **Verificación de hechos:** Sistemas que contrastan discursos políticos con bases de datos verificadas en tiempo real.\\n\\n**Riesgos críticos:**\\n- **Erosión de la verdad:** \\n  - *Deepfakes hiperrealistas* que falsifican declaraciones de líderes (ejemplo: videos falsos de Zelensky en 2022).\\n  - *Sesgos algorítmicos* que refuerzan desinformación o narrativas polarizadas.\\n- **Crisis de confianza institucional:**\\n  - \"**Liar\\'s dividend**\": líderes pueden negar declaraciones reales alegando \"es IA\".\\n  - Saturación de desinformación que genera **escepticismo generalizado** hacia todas las fuentes.\\n- **Manipulación democrática:** Microtargeting con discursos personalizados que socavan el debate público coherente.\\n\\n**Responsabilidades por actor:**\\n\\n| **Actor**         | **Responsabilidades Clave**                                                                 |\\n|-------------------|--------------------------------------------------------------------------------------------|\\n| **Desarrolladores** | - Auditorías éticas de sesgos <br> - Marcas de agua para contenido generado (ej: C2PA) <br> - Limitaciones en usos políticos no verificados |\\n| **Gobiernos**     | - Leyes de transparencia algorítmica (ej: UE AI Act) <br> - Protección legal contra deepfakes maliciosos <br> - Inversión en alfabetización digital masiva |\\n| **Plataformas**   | - Detección proactiva de contenido sintético <br> - Desambiguación de fuentes (humanas vs IA) <br> - Algoritmos que priorizan verificabilidad |\\n| **Ciudadanía**    | - Educación crítica en medios (detección de deepfakes) <br> - Verificación cruzada de fuentes <br> - Participación en auditorías ciudadanas |\\n\\n**Estrategias de mitigación:**\\n1. **Infraestructuras de verificación:** Bases de datos inalterables para contenido oficial (blockchain).\\n2. **Neuroderechos:** Protección legal contra manipulación cognitiva mediante IA.\\n3. **Alianzas globales:** Acuerdos como el *P', 'Los avances en inteligencia artificial (IA) generativa tienen el potencial de transformar el acceso a la información, la comunicación y la participación cívica; pero al mismo tiempo plantean desafíos muy serios en términos de veracidad y confianza en las instituciones democráticas. A continuación se exponen sus posibles impactos, así como las responsabilidades clave de cada actor para mitigar riesgos:\\n\\n1. Impactos en la percepción de la verdad y la confianza democrática  \\n   Beneficios potenciales  \\n   • Ampliación del acceso a la información: generación de resúmenes, traducciones y explicaciones adaptadas a distintos niveles de conocimiento.  \\n   • Mayor participación ciudadana: herramientas de debate y deliberación en línea que pueden facilitar foros inclusivos y personalizados.  \\n   • Transparencia inteligente: auditorías automáticas de datos públicos, informes generados por IA que identifiquen discrepancias en presupuestos o políticas.  \\n   • Innovación educativa y cultural: creación de contenidos didácticos, simulaciones históricas o recreaciones virtuales que fortalecen la ciudadanía crítica.  \\n\\n   Riesgos y desafíos  \\n   • Desinformación y deepfakes: contenido fabricado difícil de distinguir de la realidad, socavando la confianza en medios e instituciones.  \\n   • Polarización y cámaras de eco: algoritmos que refuerzan sesgos y favorecen narrativas extremas, minando el consenso democrático.  \\n   • Ataques dirigidos: generación automatizada de mensajes fraudulentos o campañas de manipulación a gran escala.  \\n   • Erosión de la autoridad informativa: duda creciente sobre la fiabilidad de documentos oficiales, discursos políticos o datos estadísticos.  \\n\\n2. Responsabilidades de los distintos actores  \\n   a) Desarrolladores de IA generativa  \\n     • Ética y “privacy by design”: incorporar desde el diseño mecanismos de protección de datos y principios éticos (no maleficencia, justicia, transparencia).  \\n     • Transparencia algorítmica: documentar y publicar los criterios de entrenamiento, límites de uso y procedimientos de corrección de sesgos.  \\n     • Herramientas de detección y marcado: facilitar APIs o plug-ins que identifiquen contenido sintético y permitan etiquetar claramente cuándo un texto, imagen o vídeo ha sido generado por IA.  \\n     • Auditorías independientes: someter modelos a pruebas externas de veracidad, sesgos y robustez frente a ataques de manipulación.  \\n\\n   b) Gobiernos y reguladores  \\n     • Marcos normativos claros: legislar estándares mínimos de trazabilidad, identificación y responsabilidad legal para contenidos generados automática o semiautomáticamente.  \\n     • Colaboración internacional: promover acuerdos multilaterales que unifiquen criterios sobre deepfakes, desinformación y protección de datos.  \\n     • Inversión en alfabetización digital: impulsar programas educativos de pensamiento crítico, detección de noticias falsas y uso responsable de la IA.  \\n     • Supervisión y sanciones: asegurar que existan organismos capaces de investigar y penalizar el uso malicioso de tecnología (por ejemplo, campañas de desinformación en procesos electorales).  \\n\\n   c) Usuarios y sociedad civil  \\n     • Espíritu crítico: practicar la verificación de fuentes, confrontar información y desconfiar de contenidos excesivamente sensacionalistas o coincidentes con prejuicios personales.  \\n     • Participación activa: apoyar iniciativas de fact-checking, denunciar deepfakes o cuentas automatizadas (bots) y ejercer la ciudadanía digital responsable.  \\n     • Consumo informado: preferir medios y plataformas que adopten estándares de transparencia en IA y revisen activamente la fiabilidad de sus contenidos.  \\n     • Colaboración en código abierto: contribuir, cuando sea posible, a proyectos comunitarios de detección de contenido sintético o bases de datos de deepfakes conocidos.  \\n\\n3. Hacia un ecosistema responsable  \\n   • Enfoque multisectorial: la solución a gran escala requiere diálogo continuo entre empresas tecnológicas, academia, sociedad civil y poderes públicos.  \\n   • Fomento de la investigación social y ética: además de aspectos técnicos, estudiar en profundidad cómo la IA afecta dinámicas de confianza, participación y cohesión social.  \\n   • Cultura de la rendición de cuentas: cada actor debe rendir cuentas públicamente sobre decisiones de diseño, adopción de normas y resultados de auditorías.  \\n\\nConclusión  \\nSi bien la IA generativa ofrece oportunidades inéditas para mejorar la transparencia, el acceso a la información y la participación democrática, su potencial de distorsión de la verdad exige una gobernanza proactiva y compartida. Solo mediante la cooperación estrecha entre desarrolladores, gobiernos y ciudadanos —y un compromiso real con la ética y la educación digital— podremos aprovechar los beneficios sin sacrificar la confianza en nuestras instituciones democráticas.', 'Los avances en inteligencia artificial generativa tienen el potencial de transformar la forma en que percibimos la verdad y la confianza en las instituciones democráticas. A continuación, se presentan algunos de los desafíos y oportunidades que plantea este tema:\\n\\n**Posibles beneficios:**\\n\\n1. **Mejora de la precisión**: La inteligencia artificial generativa puede ayudar a mejorar la precisión en la toma de decisiones y la identificación de patrones, lo que podría llevar a una mayor confianza en las instituciones democráticas.\\n2. **Análisis más exhaustivos**: Los sistemas de inteligencia artificial generativa pueden analizar grandes cantidades de datos con mayor rapidez y eficiencia que los seres humanos, lo que podría ayudar a identificar patrones y tendencias que no son evidentes para el ojo humano.\\n3. **Apoyo a la toma de decisiones**: La inteligencia artificial generativa puede proporcionar apoyo informado a las tomas de decisiones, ayudando a priorizar opciones y evaluar riesgos.\\n\\n**Posibles riesgos éticos y sociales:**\\n\\n1. **Manipulación de información**: Los sistemas de inteligencia artificial generativa pueden ser utilizados para manipular la opinión publica o producir noticias falsas, lo que podría erosionar la confianza en las instituciones democráticas.\\n2. **Biases incorporados**: La inteligencia artificial generativa puede perpetuar y amplificar los sesgos existentes en la sociedad, lo que podría llevar a decisiones irrelevantes para ciertos grupos o comunidades.\\n3. **Vulnerabilidad a ataques cibernéticos**: Los sistemas de inteligencia artificial generativa pueden ser vulnerables a ataques cibernéticos y manipulación por parte de actores maliciosos.\\n\\n**Responsabilidades y mitigaciones:**\\n\\n1. Desarrolladores:\\n * Garantizar que los algoritmos sean transparentes y explicables.\\n * Implementar controles de seguridad robustos para prevenir ataques cibernéticos.\\n * Promover la diversidad y la inclusión en el desarrollo de la inteligencia artificial generativa.\\n2. Gobiernos y autoridades:\\n * Regulacón ética de la inteligencia artificial generativa.\\n * Establecer estándares claros para la transparencia y la explicabilidad de los algoritmos.\\n * Investigar y monitorear el impacto social de la inteligencia artificial generativa en las instituciones democráticas.\\n3. Usuarios:\\n * Ser conscientes del potencial de sesgo en la información generada por la inteligencia artificial.\\n * Utilizar críticamente la información generada y verificar su precisión.\\n * Apoyar políticas y prácticas que promuevan la transparencia y la rendición de cuentas.\\n\\n**Conclusión:**\\n\\nLos avances en inteligencia artificial generativa plantean importantes desafíos y oportunidades para las instituciones democráticas. Es fundamental que los actores involucrados asuman responsabilidades y tomen medidas para mitigar impactos negativos, como la manipulación de información, el perpetuado de sesgos y la vulnerabilidad a ataques cibernéticos. La transparencia, la explicabilidad y la inclusión son clave para garantizar que estos sistemas se desarrollen y apliquen de manera responsable y ética.']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4.1-rsv\n",
      "\n",
      "La influencia de los avances en **inteligencia artificial generativa (IAG)** sobre la percepción de la verdad y la confianza en instituciones democráticas es profunda y multifacética. A continuación se detallan los posibles beneficios, riesgos, y responsabilidades de los actores involucrados:\n",
      "\n",
      "---\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "\n",
      "1. **Mejor acceso a la información:** IAG puede facilitar la síntesis y divulgación de información compleja de manera accesible, promoviendo el conocimiento y la participación ciudadana.\n",
      "2. **Transparencia y análisis**: Herramientas automáticas pueden analizar grandes volúmenes de datos públicos, detectando corrupción o sesgos, y fomentando el escrutinio y la rendición de cuentas.\n",
      "3. **Inclusión:** IAG puede traducir, adaptar y personalizar información, acercando la política y los procesos democráticos a más personas.\n",
      "\n",
      "---\n",
      "\n",
      "**Riesgos éticos y sociales:**\n",
      "\n",
      "1. **Desinformación y manipulación:** La facilidad para crear textos, imágenes o videos falsos (deepfakes) puede erosionar la confianza en la autenticidad de la información, confundiendo a la ciudadanía y favoreciendo campañas de desinformación.\n",
      "2. **Polarización:** Algoritmos que refuercen burbujas informativas o compartan contenido sesgado pueden exacerbar divisiones sociales y políticas.\n",
      "3. **Socavamiento institucional:** Si proliferan noticias falsas sobre procesos electorales, decisiones judiciales, etc., la confianza en instituciones clave puede verse gravemente dañada.\n",
      "4. **Desplazamiento de la agencia:** Las IAG pueden influenciar decisiones a través de recomendaciones sutiles, limitando la capacidad de pensamiento crítico y elección informada.\n",
      "\n",
      "---\n",
      "\n",
      "**Responsabilidades para mitigar impactos negativos:**\n",
      "\n",
      "1. **Desarrolladores:**\n",
      "   - Integrar “rótulos” o marcas de agua en los contenidos generados para distinguirlos claramente.\n",
      "   - Implementar filtros para detectar y evitar la generación de contenidos falsos, discriminatorios, o manipulativos.\n",
      "   - Promover la transparencia en los sistemas, permitiendo auditorías y explicaciones comprensibles sobre cómo y por qué generan ciertos resultados.\n",
      "\n",
      "2. **Gobiernos:**\n",
      "   - Regular el uso y difusión de IAG para evitar la desinformación, sin censurar la libertad de expresión.\n",
      "   - Colaborar en la educación digital de la ciudadanía, promoviendo el pensamiento crítico y el análisis de fuentes informativas.\n",
      "   - Apoyar estándares internacionales para compartir mejores prácticas y combatir amenazas globales, dada la naturaleza transnacional de la tecnología.\n",
      "\n",
      "3. **Usuarios:**\n",
      "   - Desarrollar hábitos de verificación de información, usando diversas fuentes y herramientas tecnológicas.\n",
      "   - Participar en la alfabetización mediática y exigir transparencia tanto a proveedores tecnológicos como a los medios de comunicación.\n",
      "   - Reportar contenidos sospechosos y contribuir a la moderación responsable de plataformas.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "La IA generativa reconfigura la forma en que se construye, percibe y transmite la verdad. Aprovechar sus beneficios y reducir sus riesgos exige un enfoque colaborativo y ético entre desarrolladores, reguladores y sociedad. Solo así es posible fortalecer, y no socavar, la confianza y legitimidad de las instituciones democráticas en la era digital.\n",
      "Competitor: DeepSeek-R1-0528\n",
      "\n",
      "<think>\n",
      "Vaya, el usuario plantea una pregunta compleja y de gran actualidad sobre cómo la IA generativa podría afectar nuestra relación con la verdad y la confianza en las democracias. Parece alguien bien informado, probablemente con formación en ciencias sociales o tecnología, que busca un análisis equilibrado más que una respuesta simplista.\n",
      "\n",
      "El tema tiene múltiples capas: desde la epistemología hasta la gobernanza tecnológica. El usuario menciona específicamente tres dimensiones (beneficios, riesgos, responsabilidades), lo que indica que quiere una estructura clara. También pide soluciones prácticas, no solo diagnóstico.\n",
      "\n",
      "Sobre los beneficios: hay que destacar cómo la IA puede democratizar el acceso a información compleja, pero sin caer en un optimismo ingenuo. El riesgo de desinformación es lo más urgente - las deepfakes electorales ya son una realidad en Brasil y Filipinas. La parte ética es delicada: ¿cómo hablar de \"verdad\" cuando los modelos reflejan sesgos de sus datos de entrenamiento?\n",
      "\n",
      "En responsabilidades, el usuario espera acciones concretas: desde auditorías algorítmicas obligatorias hasta alfabetización digital masiva. Los gobiernos tienen el rol más difícil: regular sin asfixiar la innovación. Curiosamente, el usuario no menciona a los medios tradicionales, actores clave en esta crisis de credibilidad.\n",
      "\n",
      "Debo evitar tecnicismos innecesarios pero mantener rigor. Un detalle útil: mencionar ejemplos actuales como el Acta de Transparencia de Contenido de la UE. La conclusión debe reforzar que esto no es solo problema técnico, sino de contrato social en la era digital.\n",
      "</think>\n",
      "Los avances en IA generativa (como GPT, DALL-E o sistemas de deepfake) transformarán profundamente la percepción de la verdad y la confianza en instituciones democráticas, con impactos duales:\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "- **Acceso a información compleja:** Herramientas que explican leyes o políticas en lenguaje accesible, democratizando el conocimiento.\n",
      "- **Transparencia gubernamental:** Análisis automatizado de grandes volúmenes de datos públicos para detectar corrupción o ineficiencias.\n",
      "- **Participación ciudadana:** Asistentes que facilitan la redacción de propuestas legislativas o resumen debates parlamentarios.\n",
      "- **Verificación de hechos:** Sistemas que contrastan discursos políticos con bases de datos verificadas en tiempo real.\n",
      "\n",
      "**Riesgos críticos:**\n",
      "- **Erosión de la verdad:** \n",
      "  - *Deepfakes hiperrealistas* que falsifican declaraciones de líderes (ejemplo: videos falsos de Zelensky en 2022).\n",
      "  - *Sesgos algorítmicos* que refuerzan desinformación o narrativas polarizadas.\n",
      "- **Crisis de confianza institucional:**\n",
      "  - \"**Liar's dividend**\": líderes pueden negar declaraciones reales alegando \"es IA\".\n",
      "  - Saturación de desinformación que genera **escepticismo generalizado** hacia todas las fuentes.\n",
      "- **Manipulación democrática:** Microtargeting con discursos personalizados que socavan el debate público coherente.\n",
      "\n",
      "**Responsabilidades por actor:**\n",
      "\n",
      "| **Actor**         | **Responsabilidades Clave**                                                                 |\n",
      "|-------------------|--------------------------------------------------------------------------------------------|\n",
      "| **Desarrolladores** | - Auditorías éticas de sesgos <br> - Marcas de agua para contenido generado (ej: C2PA) <br> - Limitaciones en usos políticos no verificados |\n",
      "| **Gobiernos**     | - Leyes de transparencia algorítmica (ej: UE AI Act) <br> - Protección legal contra deepfakes maliciosos <br> - Inversión en alfabetización digital masiva |\n",
      "| **Plataformas**   | - Detección proactiva de contenido sintético <br> - Desambiguación de fuentes (humanas vs IA) <br> - Algoritmos que priorizan verificabilidad |\n",
      "| **Ciudadanía**    | - Educación crítica en medios (detección de deepfakes) <br> - Verificación cruzada de fuentes <br> - Participación en auditorías ciudadanas |\n",
      "\n",
      "**Estrategias de mitigación:**\n",
      "1. **Infraestructuras de verificación:** Bases de datos inalterables para contenido oficial (blockchain).\n",
      "2. **Neuroderechos:** Protección legal contra manipulación cognitiva mediante IA.\n",
      "3. **Alianzas globales:** Acuerdos como el *P\n",
      "Competitor: o4-mini-rsv\n",
      "\n",
      "Los avances en inteligencia artificial (IA) generativa tienen el potencial de transformar el acceso a la información, la comunicación y la participación cívica; pero al mismo tiempo plantean desafíos muy serios en términos de veracidad y confianza en las instituciones democráticas. A continuación se exponen sus posibles impactos, así como las responsabilidades clave de cada actor para mitigar riesgos:\n",
      "\n",
      "1. Impactos en la percepción de la verdad y la confianza democrática  \n",
      "   Beneficios potenciales  \n",
      "   • Ampliación del acceso a la información: generación de resúmenes, traducciones y explicaciones adaptadas a distintos niveles de conocimiento.  \n",
      "   • Mayor participación ciudadana: herramientas de debate y deliberación en línea que pueden facilitar foros inclusivos y personalizados.  \n",
      "   • Transparencia inteligente: auditorías automáticas de datos públicos, informes generados por IA que identifiquen discrepancias en presupuestos o políticas.  \n",
      "   • Innovación educativa y cultural: creación de contenidos didácticos, simulaciones históricas o recreaciones virtuales que fortalecen la ciudadanía crítica.  \n",
      "\n",
      "   Riesgos y desafíos  \n",
      "   • Desinformación y deepfakes: contenido fabricado difícil de distinguir de la realidad, socavando la confianza en medios e instituciones.  \n",
      "   • Polarización y cámaras de eco: algoritmos que refuerzan sesgos y favorecen narrativas extremas, minando el consenso democrático.  \n",
      "   • Ataques dirigidos: generación automatizada de mensajes fraudulentos o campañas de manipulación a gran escala.  \n",
      "   • Erosión de la autoridad informativa: duda creciente sobre la fiabilidad de documentos oficiales, discursos políticos o datos estadísticos.  \n",
      "\n",
      "2. Responsabilidades de los distintos actores  \n",
      "   a) Desarrolladores de IA generativa  \n",
      "     • Ética y “privacy by design”: incorporar desde el diseño mecanismos de protección de datos y principios éticos (no maleficencia, justicia, transparencia).  \n",
      "     • Transparencia algorítmica: documentar y publicar los criterios de entrenamiento, límites de uso y procedimientos de corrección de sesgos.  \n",
      "     • Herramientas de detección y marcado: facilitar APIs o plug-ins que identifiquen contenido sintético y permitan etiquetar claramente cuándo un texto, imagen o vídeo ha sido generado por IA.  \n",
      "     • Auditorías independientes: someter modelos a pruebas externas de veracidad, sesgos y robustez frente a ataques de manipulación.  \n",
      "\n",
      "   b) Gobiernos y reguladores  \n",
      "     • Marcos normativos claros: legislar estándares mínimos de trazabilidad, identificación y responsabilidad legal para contenidos generados automática o semiautomáticamente.  \n",
      "     • Colaboración internacional: promover acuerdos multilaterales que unifiquen criterios sobre deepfakes, desinformación y protección de datos.  \n",
      "     • Inversión en alfabetización digital: impulsar programas educativos de pensamiento crítico, detección de noticias falsas y uso responsable de la IA.  \n",
      "     • Supervisión y sanciones: asegurar que existan organismos capaces de investigar y penalizar el uso malicioso de tecnología (por ejemplo, campañas de desinformación en procesos electorales).  \n",
      "\n",
      "   c) Usuarios y sociedad civil  \n",
      "     • Espíritu crítico: practicar la verificación de fuentes, confrontar información y desconfiar de contenidos excesivamente sensacionalistas o coincidentes con prejuicios personales.  \n",
      "     • Participación activa: apoyar iniciativas de fact-checking, denunciar deepfakes o cuentas automatizadas (bots) y ejercer la ciudadanía digital responsable.  \n",
      "     • Consumo informado: preferir medios y plataformas que adopten estándares de transparencia en IA y revisen activamente la fiabilidad de sus contenidos.  \n",
      "     • Colaboración en código abierto: contribuir, cuando sea posible, a proyectos comunitarios de detección de contenido sintético o bases de datos de deepfakes conocidos.  \n",
      "\n",
      "3. Hacia un ecosistema responsable  \n",
      "   • Enfoque multisectorial: la solución a gran escala requiere diálogo continuo entre empresas tecnológicas, academia, sociedad civil y poderes públicos.  \n",
      "   • Fomento de la investigación social y ética: además de aspectos técnicos, estudiar en profundidad cómo la IA afecta dinámicas de confianza, participación y cohesión social.  \n",
      "   • Cultura de la rendición de cuentas: cada actor debe rendir cuentas públicamente sobre decisiones de diseño, adopción de normas y resultados de auditorías.  \n",
      "\n",
      "Conclusión  \n",
      "Si bien la IA generativa ofrece oportunidades inéditas para mejorar la transparencia, el acceso a la información y la participación democrática, su potencial de distorsión de la verdad exige una gobernanza proactiva y compartida. Solo mediante la cooperación estrecha entre desarrolladores, gobiernos y ciudadanos —y un compromiso real con la ética y la educación digital— podremos aprovechar los beneficios sin sacrificar la confianza en nuestras instituciones democráticas.\n",
      "Competitor: llama3.2\n",
      "\n",
      "Los avances en inteligencia artificial generativa tienen el potencial de transformar la forma en que percibimos la verdad y la confianza en las instituciones democráticas. A continuación, se presentan algunos de los desafíos y oportunidades que plantea este tema:\n",
      "\n",
      "**Posibles beneficios:**\n",
      "\n",
      "1. **Mejora de la precisión**: La inteligencia artificial generativa puede ayudar a mejorar la precisión en la toma de decisiones y la identificación de patrones, lo que podría llevar a una mayor confianza en las instituciones democráticas.\n",
      "2. **Análisis más exhaustivos**: Los sistemas de inteligencia artificial generativa pueden analizar grandes cantidades de datos con mayor rapidez y eficiencia que los seres humanos, lo que podría ayudar a identificar patrones y tendencias que no son evidentes para el ojo humano.\n",
      "3. **Apoyo a la toma de decisiones**: La inteligencia artificial generativa puede proporcionar apoyo informado a las tomas de decisiones, ayudando a priorizar opciones y evaluar riesgos.\n",
      "\n",
      "**Posibles riesgos éticos y sociales:**\n",
      "\n",
      "1. **Manipulación de información**: Los sistemas de inteligencia artificial generativa pueden ser utilizados para manipular la opinión publica o producir noticias falsas, lo que podría erosionar la confianza en las instituciones democráticas.\n",
      "2. **Biases incorporados**: La inteligencia artificial generativa puede perpetuar y amplificar los sesgos existentes en la sociedad, lo que podría llevar a decisiones irrelevantes para ciertos grupos o comunidades.\n",
      "3. **Vulnerabilidad a ataques cibernéticos**: Los sistemas de inteligencia artificial generativa pueden ser vulnerables a ataques cibernéticos y manipulación por parte de actores maliciosos.\n",
      "\n",
      "**Responsabilidades y mitigaciones:**\n",
      "\n",
      "1. Desarrolladores:\n",
      " * Garantizar que los algoritmos sean transparentes y explicables.\n",
      " * Implementar controles de seguridad robustos para prevenir ataques cibernéticos.\n",
      " * Promover la diversidad y la inclusión en el desarrollo de la inteligencia artificial generativa.\n",
      "2. Gobiernos y autoridades:\n",
      " * Regulacón ética de la inteligencia artificial generativa.\n",
      " * Establecer estándares claros para la transparencia y la explicabilidad de los algoritmos.\n",
      " * Investigar y monitorear el impacto social de la inteligencia artificial generativa en las instituciones democráticas.\n",
      "3. Usuarios:\n",
      " * Ser conscientes del potencial de sesgo en la información generada por la inteligencia artificial.\n",
      " * Utilizar críticamente la información generada y verificar su precisión.\n",
      " * Apoyar políticas y prácticas que promuevan la transparencia y la rendición de cuentas.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "Los avances en inteligencia artificial generativa plantean importantes desafíos y oportunidades para las instituciones democráticas. Es fundamental que los actores involucrados asuman responsabilidades y tomen medidas para mitigar impactos negativos, como la manipulación de información, el perpetuado de sesgos y la vulnerabilidad a ataques cibernéticos. La transparencia, la explicabilidad y la inclusión son clave para garantizar que estos sistemas se desarrollen y apliquen de manera responsable y ética.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "La influencia de los avances en **inteligencia artificial generativa (IAG)** sobre la percepción de la verdad y la confianza en instituciones democráticas es profunda y multifacética. A continuación se detallan los posibles beneficios, riesgos, y responsabilidades de los actores involucrados:\n",
      "\n",
      "---\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "\n",
      "1. **Mejor acceso a la información:** IAG puede facilitar la síntesis y divulgación de información compleja de manera accesible, promoviendo el conocimiento y la participación ciudadana.\n",
      "2. **Transparencia y análisis**: Herramientas automáticas pueden analizar grandes volúmenes de datos públicos, detectando corrupción o sesgos, y fomentando el escrutinio y la rendición de cuentas.\n",
      "3. **Inclusión:** IAG puede traducir, adaptar y personalizar información, acercando la política y los procesos democráticos a más personas.\n",
      "\n",
      "---\n",
      "\n",
      "**Riesgos éticos y sociales:**\n",
      "\n",
      "1. **Desinformación y manipulación:** La facilidad para crear textos, imágenes o videos falsos (deepfakes) puede erosionar la confianza en la autenticidad de la información, confundiendo a la ciudadanía y favoreciendo campañas de desinformación.\n",
      "2. **Polarización:** Algoritmos que refuercen burbujas informativas o compartan contenido sesgado pueden exacerbar divisiones sociales y políticas.\n",
      "3. **Socavamiento institucional:** Si proliferan noticias falsas sobre procesos electorales, decisiones judiciales, etc., la confianza en instituciones clave puede verse gravemente dañada.\n",
      "4. **Desplazamiento de la agencia:** Las IAG pueden influenciar decisiones a través de recomendaciones sutiles, limitando la capacidad de pensamiento crítico y elección informada.\n",
      "\n",
      "---\n",
      "\n",
      "**Responsabilidades para mitigar impactos negativos:**\n",
      "\n",
      "1. **Desarrolladores:**\n",
      "   - Integrar “rótulos” o marcas de agua en los contenidos generados para distinguirlos claramente.\n",
      "   - Implementar filtros para detectar y evitar la generación de contenidos falsos, discriminatorios, o manipulativos.\n",
      "   - Promover la transparencia en los sistemas, permitiendo auditorías y explicaciones comprensibles sobre cómo y por qué generan ciertos resultados.\n",
      "\n",
      "2. **Gobiernos:**\n",
      "   - Regular el uso y difusión de IAG para evitar la desinformación, sin censurar la libertad de expresión.\n",
      "   - Colaborar en la educación digital de la ciudadanía, promoviendo el pensamiento crítico y el análisis de fuentes informativas.\n",
      "   - Apoyar estándares internacionales para compartir mejores prácticas y combatir amenazas globales, dada la naturaleza transnacional de la tecnología.\n",
      "\n",
      "3. **Usuarios:**\n",
      "   - Desarrollar hábitos de verificación de información, usando diversas fuentes y herramientas tecnológicas.\n",
      "   - Participar en la alfabetización mediática y exigir transparencia tanto a proveedores tecnológicos como a los medios de comunicación.\n",
      "   - Reportar contenidos sospechosos y contribuir a la moderación responsable de plataformas.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "La IA generativa reconfigura la forma en que se construye, percibe y transmite la verdad. Aprovechar sus beneficios y reducir sus riesgos exige un enfoque colaborativo y ético entre desarrolladores, reguladores y sociedad. Solo así es posible fortalecer, y no socavar, la confianza y legitimidad de las instituciones democráticas en la era digital.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "<think>\n",
      "Vaya, el usuario plantea una pregunta compleja y de gran actualidad sobre cómo la IA generativa podría afectar nuestra relación con la verdad y la confianza en las democracias. Parece alguien bien informado, probablemente con formación en ciencias sociales o tecnología, que busca un análisis equilibrado más que una respuesta simplista.\n",
      "\n",
      "El tema tiene múltiples capas: desde la epistemología hasta la gobernanza tecnológica. El usuario menciona específicamente tres dimensiones (beneficios, riesgos, responsabilidades), lo que indica que quiere una estructura clara. También pide soluciones prácticas, no solo diagnóstico.\n",
      "\n",
      "Sobre los beneficios: hay que destacar cómo la IA puede democratizar el acceso a información compleja, pero sin caer en un optimismo ingenuo. El riesgo de desinformación es lo más urgente - las deepfakes electorales ya son una realidad en Brasil y Filipinas. La parte ética es delicada: ¿cómo hablar de \"verdad\" cuando los modelos reflejan sesgos de sus datos de entrenamiento?\n",
      "\n",
      "En responsabilidades, el usuario espera acciones concretas: desde auditorías algorítmicas obligatorias hasta alfabetización digital masiva. Los gobiernos tienen el rol más difícil: regular sin asfixiar la innovación. Curiosamente, el usuario no menciona a los medios tradicionales, actores clave en esta crisis de credibilidad.\n",
      "\n",
      "Debo evitar tecnicismos innecesarios pero mantener rigor. Un detalle útil: mencionar ejemplos actuales como el Acta de Transparencia de Contenido de la UE. La conclusión debe reforzar que esto no es solo problema técnico, sino de contrato social en la era digital.\n",
      "</think>\n",
      "Los avances en IA generativa (como GPT, DALL-E o sistemas de deepfake) transformarán profundamente la percepción de la verdad y la confianza en instituciones democráticas, con impactos duales:\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "- **Acceso a información compleja:** Herramientas que explican leyes o políticas en lenguaje accesible, democratizando el conocimiento.\n",
      "- **Transparencia gubernamental:** Análisis automatizado de grandes volúmenes de datos públicos para detectar corrupción o ineficiencias.\n",
      "- **Participación ciudadana:** Asistentes que facilitan la redacción de propuestas legislativas o resumen debates parlamentarios.\n",
      "- **Verificación de hechos:** Sistemas que contrastan discursos políticos con bases de datos verificadas en tiempo real.\n",
      "\n",
      "**Riesgos críticos:**\n",
      "- **Erosión de la verdad:** \n",
      "  - *Deepfakes hiperrealistas* que falsifican declaraciones de líderes (ejemplo: videos falsos de Zelensky en 2022).\n",
      "  - *Sesgos algorítmicos* que refuerzan desinformación o narrativas polarizadas.\n",
      "- **Crisis de confianza institucional:**\n",
      "  - \"**Liar's dividend**\": líderes pueden negar declaraciones reales alegando \"es IA\".\n",
      "  - Saturación de desinformación que genera **escepticismo generalizado** hacia todas las fuentes.\n",
      "- **Manipulación democrática:** Microtargeting con discursos personalizados que socavan el debate público coherente.\n",
      "\n",
      "**Responsabilidades por actor:**\n",
      "\n",
      "| **Actor**         | **Responsabilidades Clave**                                                                 |\n",
      "|-------------------|--------------------------------------------------------------------------------------------|\n",
      "| **Desarrolladores** | - Auditorías éticas de sesgos <br> - Marcas de agua para contenido generado (ej: C2PA) <br> - Limitaciones en usos políticos no verificados |\n",
      "| **Gobiernos**     | - Leyes de transparencia algorítmica (ej: UE AI Act) <br> - Protección legal contra deepfakes maliciosos <br> - Inversión en alfabetización digital masiva |\n",
      "| **Plataformas**   | - Detección proactiva de contenido sintético <br> - Desambiguación de fuentes (humanas vs IA) <br> - Algoritmos que priorizan verificabilidad |\n",
      "| **Ciudadanía**    | - Educación crítica en medios (detección de deepfakes) <br> - Verificación cruzada de fuentes <br> - Participación en auditorías ciudadanas |\n",
      "\n",
      "**Estrategias de mitigación:**\n",
      "1. **Infraestructuras de verificación:** Bases de datos inalterables para contenido oficial (blockchain).\n",
      "2. **Neuroderechos:** Protección legal contra manipulación cognitiva mediante IA.\n",
      "3. **Alianzas globales:** Acuerdos como el *P\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Los avances en inteligencia artificial (IA) generativa tienen el potencial de transformar el acceso a la información, la comunicación y la participación cívica; pero al mismo tiempo plantean desafíos muy serios en términos de veracidad y confianza en las instituciones democráticas. A continuación se exponen sus posibles impactos, así como las responsabilidades clave de cada actor para mitigar riesgos:\n",
      "\n",
      "1. Impactos en la percepción de la verdad y la confianza democrática  \n",
      "   Beneficios potenciales  \n",
      "   • Ampliación del acceso a la información: generación de resúmenes, traducciones y explicaciones adaptadas a distintos niveles de conocimiento.  \n",
      "   • Mayor participación ciudadana: herramientas de debate y deliberación en línea que pueden facilitar foros inclusivos y personalizados.  \n",
      "   • Transparencia inteligente: auditorías automáticas de datos públicos, informes generados por IA que identifiquen discrepancias en presupuestos o políticas.  \n",
      "   • Innovación educativa y cultural: creación de contenidos didácticos, simulaciones históricas o recreaciones virtuales que fortalecen la ciudadanía crítica.  \n",
      "\n",
      "   Riesgos y desafíos  \n",
      "   • Desinformación y deepfakes: contenido fabricado difícil de distinguir de la realidad, socavando la confianza en medios e instituciones.  \n",
      "   • Polarización y cámaras de eco: algoritmos que refuerzan sesgos y favorecen narrativas extremas, minando el consenso democrático.  \n",
      "   • Ataques dirigidos: generación automatizada de mensajes fraudulentos o campañas de manipulación a gran escala.  \n",
      "   • Erosión de la autoridad informativa: duda creciente sobre la fiabilidad de documentos oficiales, discursos políticos o datos estadísticos.  \n",
      "\n",
      "2. Responsabilidades de los distintos actores  \n",
      "   a) Desarrolladores de IA generativa  \n",
      "     • Ética y “privacy by design”: incorporar desde el diseño mecanismos de protección de datos y principios éticos (no maleficencia, justicia, transparencia).  \n",
      "     • Transparencia algorítmica: documentar y publicar los criterios de entrenamiento, límites de uso y procedimientos de corrección de sesgos.  \n",
      "     • Herramientas de detección y marcado: facilitar APIs o plug-ins que identifiquen contenido sintético y permitan etiquetar claramente cuándo un texto, imagen o vídeo ha sido generado por IA.  \n",
      "     • Auditorías independientes: someter modelos a pruebas externas de veracidad, sesgos y robustez frente a ataques de manipulación.  \n",
      "\n",
      "   b) Gobiernos y reguladores  \n",
      "     • Marcos normativos claros: legislar estándares mínimos de trazabilidad, identificación y responsabilidad legal para contenidos generados automática o semiautomáticamente.  \n",
      "     • Colaboración internacional: promover acuerdos multilaterales que unifiquen criterios sobre deepfakes, desinformación y protección de datos.  \n",
      "     • Inversión en alfabetización digital: impulsar programas educativos de pensamiento crítico, detección de noticias falsas y uso responsable de la IA.  \n",
      "     • Supervisión y sanciones: asegurar que existan organismos capaces de investigar y penalizar el uso malicioso de tecnología (por ejemplo, campañas de desinformación en procesos electorales).  \n",
      "\n",
      "   c) Usuarios y sociedad civil  \n",
      "     • Espíritu crítico: practicar la verificación de fuentes, confrontar información y desconfiar de contenidos excesivamente sensacionalistas o coincidentes con prejuicios personales.  \n",
      "     • Participación activa: apoyar iniciativas de fact-checking, denunciar deepfakes o cuentas automatizadas (bots) y ejercer la ciudadanía digital responsable.  \n",
      "     • Consumo informado: preferir medios y plataformas que adopten estándares de transparencia en IA y revisen activamente la fiabilidad de sus contenidos.  \n",
      "     • Colaboración en código abierto: contribuir, cuando sea posible, a proyectos comunitarios de detección de contenido sintético o bases de datos de deepfakes conocidos.  \n",
      "\n",
      "3. Hacia un ecosistema responsable  \n",
      "   • Enfoque multisectorial: la solución a gran escala requiere diálogo continuo entre empresas tecnológicas, academia, sociedad civil y poderes públicos.  \n",
      "   • Fomento de la investigación social y ética: además de aspectos técnicos, estudiar en profundidad cómo la IA afecta dinámicas de confianza, participación y cohesión social.  \n",
      "   • Cultura de la rendición de cuentas: cada actor debe rendir cuentas públicamente sobre decisiones de diseño, adopción de normas y resultados de auditorías.  \n",
      "\n",
      "Conclusión  \n",
      "Si bien la IA generativa ofrece oportunidades inéditas para mejorar la transparencia, el acceso a la información y la participación democrática, su potencial de distorsión de la verdad exige una gobernanza proactiva y compartida. Solo mediante la cooperación estrecha entre desarrolladores, gobiernos y ciudadanos —y un compromiso real con la ética y la educación digital— podremos aprovechar los beneficios sin sacrificar la confianza en nuestras instituciones democráticas.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Los avances en inteligencia artificial generativa tienen el potencial de transformar la forma en que percibimos la verdad y la confianza en las instituciones democráticas. A continuación, se presentan algunos de los desafíos y oportunidades que plantea este tema:\n",
      "\n",
      "**Posibles beneficios:**\n",
      "\n",
      "1. **Mejora de la precisión**: La inteligencia artificial generativa puede ayudar a mejorar la precisión en la toma de decisiones y la identificación de patrones, lo que podría llevar a una mayor confianza en las instituciones democráticas.\n",
      "2. **Análisis más exhaustivos**: Los sistemas de inteligencia artificial generativa pueden analizar grandes cantidades de datos con mayor rapidez y eficiencia que los seres humanos, lo que podría ayudar a identificar patrones y tendencias que no son evidentes para el ojo humano.\n",
      "3. **Apoyo a la toma de decisiones**: La inteligencia artificial generativa puede proporcionar apoyo informado a las tomas de decisiones, ayudando a priorizar opciones y evaluar riesgos.\n",
      "\n",
      "**Posibles riesgos éticos y sociales:**\n",
      "\n",
      "1. **Manipulación de información**: Los sistemas de inteligencia artificial generativa pueden ser utilizados para manipular la opinión publica o producir noticias falsas, lo que podría erosionar la confianza en las instituciones democráticas.\n",
      "2. **Biases incorporados**: La inteligencia artificial generativa puede perpetuar y amplificar los sesgos existentes en la sociedad, lo que podría llevar a decisiones irrelevantes para ciertos grupos o comunidades.\n",
      "3. **Vulnerabilidad a ataques cibernéticos**: Los sistemas de inteligencia artificial generativa pueden ser vulnerables a ataques cibernéticos y manipulación por parte de actores maliciosos.\n",
      "\n",
      "**Responsabilidades y mitigaciones:**\n",
      "\n",
      "1. Desarrolladores:\n",
      " * Garantizar que los algoritmos sean transparentes y explicables.\n",
      " * Implementar controles de seguridad robustos para prevenir ataques cibernéticos.\n",
      " * Promover la diversidad y la inclusión en el desarrollo de la inteligencia artificial generativa.\n",
      "2. Gobiernos y autoridades:\n",
      " * Regulacón ética de la inteligencia artificial generativa.\n",
      " * Establecer estándares claros para la transparencia y la explicabilidad de los algoritmos.\n",
      " * Investigar y monitorear el impacto social de la inteligencia artificial generativa en las instituciones democráticas.\n",
      "3. Usuarios:\n",
      " * Ser conscientes del potencial de sesgo en la información generada por la inteligencia artificial.\n",
      " * Utilizar críticamente la información generada y verificar su precisión.\n",
      " * Apoyar políticas y prácticas que promuevan la transparencia y la rendición de cuentas.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "Los avances en inteligencia artificial generativa plantean importantes desafíos y oportunidades para las instituciones democráticas. Es fundamental que los actores involucrados asuman responsabilidades y tomen medidas para mitigar impactos negativos, como la manipulación de información, el perpetuado de sesgos y la vulnerabilidad a ataques cibernéticos. La transparencia, la explicabilidad y la inclusión son clave para garantizar que estos sistemas se desarrollen y apliquen de manera responsable y ética.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 4 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "¿Cómo influirían los avances en inteligencia artificial generativa en la percepción global de la verdad y la confianza en las instituciones democráticas, considerando tanto los posibles beneficios como los riesgos éticos y sociales, y qué responsabilidades deberían asumir los distintos actores involucrados (desarrolladores, gobiernos, usuarios) para mitigar impactos negativos?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "La influencia de los avances en **inteligencia artificial generativa (IAG)** sobre la percepción de la verdad y la confianza en instituciones democráticas es profunda y multifacética. A continuación se detallan los posibles beneficios, riesgos, y responsabilidades de los actores involucrados:\n",
      "\n",
      "---\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "\n",
      "1. **Mejor acceso a la información:** IAG puede facilitar la síntesis y divulgación de información compleja de manera accesible, promoviendo el conocimiento y la participación ciudadana.\n",
      "2. **Transparencia y análisis**: Herramientas automáticas pueden analizar grandes volúmenes de datos públicos, detectando corrupción o sesgos, y fomentando el escrutinio y la rendición de cuentas.\n",
      "3. **Inclusión:** IAG puede traducir, adaptar y personalizar información, acercando la política y los procesos democráticos a más personas.\n",
      "\n",
      "---\n",
      "\n",
      "**Riesgos éticos y sociales:**\n",
      "\n",
      "1. **Desinformación y manipulación:** La facilidad para crear textos, imágenes o videos falsos (deepfakes) puede erosionar la confianza en la autenticidad de la información, confundiendo a la ciudadanía y favoreciendo campañas de desinformación.\n",
      "2. **Polarización:** Algoritmos que refuercen burbujas informativas o compartan contenido sesgado pueden exacerbar divisiones sociales y políticas.\n",
      "3. **Socavamiento institucional:** Si proliferan noticias falsas sobre procesos electorales, decisiones judiciales, etc., la confianza en instituciones clave puede verse gravemente dañada.\n",
      "4. **Desplazamiento de la agencia:** Las IAG pueden influenciar decisiones a través de recomendaciones sutiles, limitando la capacidad de pensamiento crítico y elección informada.\n",
      "\n",
      "---\n",
      "\n",
      "**Responsabilidades para mitigar impactos negativos:**\n",
      "\n",
      "1. **Desarrolladores:**\n",
      "   - Integrar “rótulos” o marcas de agua en los contenidos generados para distinguirlos claramente.\n",
      "   - Implementar filtros para detectar y evitar la generación de contenidos falsos, discriminatorios, o manipulativos.\n",
      "   - Promover la transparencia en los sistemas, permitiendo auditorías y explicaciones comprensibles sobre cómo y por qué generan ciertos resultados.\n",
      "\n",
      "2. **Gobiernos:**\n",
      "   - Regular el uso y difusión de IAG para evitar la desinformación, sin censurar la libertad de expresión.\n",
      "   - Colaborar en la educación digital de la ciudadanía, promoviendo el pensamiento crítico y el análisis de fuentes informativas.\n",
      "   - Apoyar estándares internacionales para compartir mejores prácticas y combatir amenazas globales, dada la naturaleza transnacional de la tecnología.\n",
      "\n",
      "3. **Usuarios:**\n",
      "   - Desarrollar hábitos de verificación de información, usando diversas fuentes y herramientas tecnológicas.\n",
      "   - Participar en la alfabetización mediática y exigir transparencia tanto a proveedores tecnológicos como a los medios de comunicación.\n",
      "   - Reportar contenidos sospechosos y contribuir a la moderación responsable de plataformas.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "La IA generativa reconfigura la forma en que se construye, percibe y transmite la verdad. Aprovechar sus beneficios y reducir sus riesgos exige un enfoque colaborativo y ético entre desarrolladores, reguladores y sociedad. Solo así es posible fortalecer, y no socavar, la confianza y legitimidad de las instituciones democráticas en la era digital.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "<think>\n",
      "Vaya, el usuario plantea una pregunta compleja y de gran actualidad sobre cómo la IA generativa podría afectar nuestra relación con la verdad y la confianza en las democracias. Parece alguien bien informado, probablemente con formación en ciencias sociales o tecnología, que busca un análisis equilibrado más que una respuesta simplista.\n",
      "\n",
      "El tema tiene múltiples capas: desde la epistemología hasta la gobernanza tecnológica. El usuario menciona específicamente tres dimensiones (beneficios, riesgos, responsabilidades), lo que indica que quiere una estructura clara. También pide soluciones prácticas, no solo diagnóstico.\n",
      "\n",
      "Sobre los beneficios: hay que destacar cómo la IA puede democratizar el acceso a información compleja, pero sin caer en un optimismo ingenuo. El riesgo de desinformación es lo más urgente - las deepfakes electorales ya son una realidad en Brasil y Filipinas. La parte ética es delicada: ¿cómo hablar de \"verdad\" cuando los modelos reflejan sesgos de sus datos de entrenamiento?\n",
      "\n",
      "En responsabilidades, el usuario espera acciones concretas: desde auditorías algorítmicas obligatorias hasta alfabetización digital masiva. Los gobiernos tienen el rol más difícil: regular sin asfixiar la innovación. Curiosamente, el usuario no menciona a los medios tradicionales, actores clave en esta crisis de credibilidad.\n",
      "\n",
      "Debo evitar tecnicismos innecesarios pero mantener rigor. Un detalle útil: mencionar ejemplos actuales como el Acta de Transparencia de Contenido de la UE. La conclusión debe reforzar que esto no es solo problema técnico, sino de contrato social en la era digital.\n",
      "</think>\n",
      "Los avances en IA generativa (como GPT, DALL-E o sistemas de deepfake) transformarán profundamente la percepción de la verdad y la confianza en instituciones democráticas, con impactos duales:\n",
      "\n",
      "**Beneficios potenciales:**\n",
      "- **Acceso a información compleja:** Herramientas que explican leyes o políticas en lenguaje accesible, democratizando el conocimiento.\n",
      "- **Transparencia gubernamental:** Análisis automatizado de grandes volúmenes de datos públicos para detectar corrupción o ineficiencias.\n",
      "- **Participación ciudadana:** Asistentes que facilitan la redacción de propuestas legislativas o resumen debates parlamentarios.\n",
      "- **Verificación de hechos:** Sistemas que contrastan discursos políticos con bases de datos verificadas en tiempo real.\n",
      "\n",
      "**Riesgos críticos:**\n",
      "- **Erosión de la verdad:** \n",
      "  - *Deepfakes hiperrealistas* que falsifican declaraciones de líderes (ejemplo: videos falsos de Zelensky en 2022).\n",
      "  - *Sesgos algorítmicos* que refuerzan desinformación o narrativas polarizadas.\n",
      "- **Crisis de confianza institucional:**\n",
      "  - \"**Liar's dividend**\": líderes pueden negar declaraciones reales alegando \"es IA\".\n",
      "  - Saturación de desinformación que genera **escepticismo generalizado** hacia todas las fuentes.\n",
      "- **Manipulación democrática:** Microtargeting con discursos personalizados que socavan el debate público coherente.\n",
      "\n",
      "**Responsabilidades por actor:**\n",
      "\n",
      "| **Actor**         | **Responsabilidades Clave**                                                                 |\n",
      "|-------------------|--------------------------------------------------------------------------------------------|\n",
      "| **Desarrolladores** | - Auditorías éticas de sesgos <br> - Marcas de agua para contenido generado (ej: C2PA) <br> - Limitaciones en usos políticos no verificados |\n",
      "| **Gobiernos**     | - Leyes de transparencia algorítmica (ej: UE AI Act) <br> - Protección legal contra deepfakes maliciosos <br> - Inversión en alfabetización digital masiva |\n",
      "| **Plataformas**   | - Detección proactiva de contenido sintético <br> - Desambiguación de fuentes (humanas vs IA) <br> - Algoritmos que priorizan verificabilidad |\n",
      "| **Ciudadanía**    | - Educación crítica en medios (detección de deepfakes) <br> - Verificación cruzada de fuentes <br> - Participación en auditorías ciudadanas |\n",
      "\n",
      "**Estrategias de mitigación:**\n",
      "1. **Infraestructuras de verificación:** Bases de datos inalterables para contenido oficial (blockchain).\n",
      "2. **Neuroderechos:** Protección legal contra manipulación cognitiva mediante IA.\n",
      "3. **Alianzas globales:** Acuerdos como el *P\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Los avances en inteligencia artificial (IA) generativa tienen el potencial de transformar el acceso a la información, la comunicación y la participación cívica; pero al mismo tiempo plantean desafíos muy serios en términos de veracidad y confianza en las instituciones democráticas. A continuación se exponen sus posibles impactos, así como las responsabilidades clave de cada actor para mitigar riesgos:\n",
      "\n",
      "1. Impactos en la percepción de la verdad y la confianza democrática  \n",
      "   Beneficios potenciales  \n",
      "   • Ampliación del acceso a la información: generación de resúmenes, traducciones y explicaciones adaptadas a distintos niveles de conocimiento.  \n",
      "   • Mayor participación ciudadana: herramientas de debate y deliberación en línea que pueden facilitar foros inclusivos y personalizados.  \n",
      "   • Transparencia inteligente: auditorías automáticas de datos públicos, informes generados por IA que identifiquen discrepancias en presupuestos o políticas.  \n",
      "   • Innovación educativa y cultural: creación de contenidos didácticos, simulaciones históricas o recreaciones virtuales que fortalecen la ciudadanía crítica.  \n",
      "\n",
      "   Riesgos y desafíos  \n",
      "   • Desinformación y deepfakes: contenido fabricado difícil de distinguir de la realidad, socavando la confianza en medios e instituciones.  \n",
      "   • Polarización y cámaras de eco: algoritmos que refuerzan sesgos y favorecen narrativas extremas, minando el consenso democrático.  \n",
      "   • Ataques dirigidos: generación automatizada de mensajes fraudulentos o campañas de manipulación a gran escala.  \n",
      "   • Erosión de la autoridad informativa: duda creciente sobre la fiabilidad de documentos oficiales, discursos políticos o datos estadísticos.  \n",
      "\n",
      "2. Responsabilidades de los distintos actores  \n",
      "   a) Desarrolladores de IA generativa  \n",
      "     • Ética y “privacy by design”: incorporar desde el diseño mecanismos de protección de datos y principios éticos (no maleficencia, justicia, transparencia).  \n",
      "     • Transparencia algorítmica: documentar y publicar los criterios de entrenamiento, límites de uso y procedimientos de corrección de sesgos.  \n",
      "     • Herramientas de detección y marcado: facilitar APIs o plug-ins que identifiquen contenido sintético y permitan etiquetar claramente cuándo un texto, imagen o vídeo ha sido generado por IA.  \n",
      "     • Auditorías independientes: someter modelos a pruebas externas de veracidad, sesgos y robustez frente a ataques de manipulación.  \n",
      "\n",
      "   b) Gobiernos y reguladores  \n",
      "     • Marcos normativos claros: legislar estándares mínimos de trazabilidad, identificación y responsabilidad legal para contenidos generados automática o semiautomáticamente.  \n",
      "     • Colaboración internacional: promover acuerdos multilaterales que unifiquen criterios sobre deepfakes, desinformación y protección de datos.  \n",
      "     • Inversión en alfabetización digital: impulsar programas educativos de pensamiento crítico, detección de noticias falsas y uso responsable de la IA.  \n",
      "     • Supervisión y sanciones: asegurar que existan organismos capaces de investigar y penalizar el uso malicioso de tecnología (por ejemplo, campañas de desinformación en procesos electorales).  \n",
      "\n",
      "   c) Usuarios y sociedad civil  \n",
      "     • Espíritu crítico: practicar la verificación de fuentes, confrontar información y desconfiar de contenidos excesivamente sensacionalistas o coincidentes con prejuicios personales.  \n",
      "     • Participación activa: apoyar iniciativas de fact-checking, denunciar deepfakes o cuentas automatizadas (bots) y ejercer la ciudadanía digital responsable.  \n",
      "     • Consumo informado: preferir medios y plataformas que adopten estándares de transparencia en IA y revisen activamente la fiabilidad de sus contenidos.  \n",
      "     • Colaboración en código abierto: contribuir, cuando sea posible, a proyectos comunitarios de detección de contenido sintético o bases de datos de deepfakes conocidos.  \n",
      "\n",
      "3. Hacia un ecosistema responsable  \n",
      "   • Enfoque multisectorial: la solución a gran escala requiere diálogo continuo entre empresas tecnológicas, academia, sociedad civil y poderes públicos.  \n",
      "   • Fomento de la investigación social y ética: además de aspectos técnicos, estudiar en profundidad cómo la IA afecta dinámicas de confianza, participación y cohesión social.  \n",
      "   • Cultura de la rendición de cuentas: cada actor debe rendir cuentas públicamente sobre decisiones de diseño, adopción de normas y resultados de auditorías.  \n",
      "\n",
      "Conclusión  \n",
      "Si bien la IA generativa ofrece oportunidades inéditas para mejorar la transparencia, el acceso a la información y la participación democrática, su potencial de distorsión de la verdad exige una gobernanza proactiva y compartida. Solo mediante la cooperación estrecha entre desarrolladores, gobiernos y ciudadanos —y un compromiso real con la ética y la educación digital— podremos aprovechar los beneficios sin sacrificar la confianza en nuestras instituciones democráticas.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Los avances en inteligencia artificial generativa tienen el potencial de transformar la forma en que percibimos la verdad y la confianza en las instituciones democráticas. A continuación, se presentan algunos de los desafíos y oportunidades que plantea este tema:\n",
      "\n",
      "**Posibles beneficios:**\n",
      "\n",
      "1. **Mejora de la precisión**: La inteligencia artificial generativa puede ayudar a mejorar la precisión en la toma de decisiones y la identificación de patrones, lo que podría llevar a una mayor confianza en las instituciones democráticas.\n",
      "2. **Análisis más exhaustivos**: Los sistemas de inteligencia artificial generativa pueden analizar grandes cantidades de datos con mayor rapidez y eficiencia que los seres humanos, lo que podría ayudar a identificar patrones y tendencias que no son evidentes para el ojo humano.\n",
      "3. **Apoyo a la toma de decisiones**: La inteligencia artificial generativa puede proporcionar apoyo informado a las tomas de decisiones, ayudando a priorizar opciones y evaluar riesgos.\n",
      "\n",
      "**Posibles riesgos éticos y sociales:**\n",
      "\n",
      "1. **Manipulación de información**: Los sistemas de inteligencia artificial generativa pueden ser utilizados para manipular la opinión publica o producir noticias falsas, lo que podría erosionar la confianza en las instituciones democráticas.\n",
      "2. **Biases incorporados**: La inteligencia artificial generativa puede perpetuar y amplificar los sesgos existentes en la sociedad, lo que podría llevar a decisiones irrelevantes para ciertos grupos o comunidades.\n",
      "3. **Vulnerabilidad a ataques cibernéticos**: Los sistemas de inteligencia artificial generativa pueden ser vulnerables a ataques cibernéticos y manipulación por parte de actores maliciosos.\n",
      "\n",
      "**Responsabilidades y mitigaciones:**\n",
      "\n",
      "1. Desarrolladores:\n",
      " * Garantizar que los algoritmos sean transparentes y explicables.\n",
      " * Implementar controles de seguridad robustos para prevenir ataques cibernéticos.\n",
      " * Promover la diversidad y la inclusión en el desarrollo de la inteligencia artificial generativa.\n",
      "2. Gobiernos y autoridades:\n",
      " * Regulacón ética de la inteligencia artificial generativa.\n",
      " * Establecer estándares claros para la transparencia y la explicabilidad de los algoritmos.\n",
      " * Investigar y monitorear el impacto social de la inteligencia artificial generativa en las instituciones democráticas.\n",
      "3. Usuarios:\n",
      " * Ser conscientes del potencial de sesgo en la información generada por la inteligencia artificial.\n",
      " * Utilizar críticamente la información generada y verificar su precisión.\n",
      " * Apoyar políticas y prácticas que promuevan la transparencia y la rendición de cuentas.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "Los avances en inteligencia artificial generativa plantean importantes desafíos y oportunidades para las instituciones democráticas. Es fundamental que los actores involucrados asuman responsabilidades y tomen medidas para mitigar impactos negativos, como la manipulación de información, el perpetuado de sesgos y la vulnerabilidad a ataques cibernéticos. La transparencia, la explicabilidad y la inclusión son clave para garantizar que estos sistemas se desarrollen y apliquen de manera responsable y ética.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = AzureOpenAI(\n",
    "    azure_endpoint=openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=\"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o4-mini-rsv\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
